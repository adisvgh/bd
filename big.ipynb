{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRJzfH0bTvfH"
   },
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqQqUtWyTu3m",
    "outputId": "ab617883-a9c2-44df-e255-dfa51f7bff6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "     -------------------------------------- 317.3/317.3 MB 3.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ------------------------------------ 200.5/200.5 kB 715.2 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840653 sha256=1eca8f6cc21518c0fd962cb28c7b12eb6675a26792b692c6647942ce5dc9bcd9\n",
      "  Stored in directory: c:\\users\\aditya\\appdata\\local\\pip\\cache\\wheels\\2e\\d2\\18\\6f4f20e8332359f7fffceb6828edcc80ef96f86744192a7bb9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/diabaties.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17016\\3160035806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Read the CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmy_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/diabaties.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Define the schema for the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/diabaties.csv."
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "\n",
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pyspark.sql.types as tp\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DiabetesPredictionPipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file\n",
    "my_data = spark.read.csv('/content/diabaties.csv', header=True)\n",
    "\n",
    "# Define the schema for the data\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name='Pregnancies', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Glucose', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='BloodPressure', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='SkinThickness', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Insulin', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='BMI', dataType=tp.DoubleType(), nullable=True),\n",
    "    tp.StructField(name='DiabetesPedigreeFunction', dataType=tp.DoubleType(), nullable=True),\n",
    "    tp.StructField(name='Age', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Outcome', dataType=tp.IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Read the data again with the defined schema\n",
    "my_data = spark.read.csv('/content/diabaties.csv', schema=my_schema, header=True)\n",
    "\n",
    "# Print the schema\n",
    "my_data.printSchema()\n",
    "\n",
    "# Define stages for the pipeline\n",
    "imputer = Imputer(\n",
    "    inputCols=my_data.columns,\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in my_data.columns]\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "               'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Outcome', maxIter=10)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[imputer, assembler, lr])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "xtrain, xtest = my_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline_model = pipeline.fit(xtrain)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = pipeline_model.transform(xtest)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBCDz7enU6eX"
   },
   "source": [
    "K-Shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGsOWM7TRs_W",
    "outputId": "212593c3-d8b1-4fe7-a9a2-37bcef00cdeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incidence Matrix:\n",
      "                                             /content/shingle1.txt  \\\n",
      "combat crime and so on\".[8]                                      0   \n",
      "of data now available are                                        0   \n",
      "sometimes loosely partly due to                                  1   \n",
      "for large enterprises is determining                             0   \n",
      "many entries (rows) offer greater                                1   \n",
      "...                                                            ...   \n",
      "estimated to reach $215.7 billion                                0   \n",
      "or insightfulness of the data.[5]                                1   \n",
      "data presents challenges in sampling,                            1   \n",
      "devices, aerial (remote sensing) equipment,                      0   \n",
      "analytics methods that extract value                             0   \n",
      "\n",
      "                                             /content/shingle2.txt  \\\n",
      "combat crime and so on\".[8]                                      1   \n",
      "of data now available are                                        1   \n",
      "sometimes loosely partly due to                                  0   \n",
      "for large enterprises is determining                             0   \n",
      "many entries (rows) offer greater                                0   \n",
      "...                                                            ...   \n",
      "estimated to reach $215.7 billion                                0   \n",
      "or insightfulness of the data.[5]                                1   \n",
      "data presents challenges in sampling,                            1   \n",
      "devices, aerial (remote sensing) equipment,                      0   \n",
      "analytics methods that extract value                             1   \n",
      "\n",
      "                                             /content/shingle3.txt  \n",
      "combat crime and so on\".[8]                                      1  \n",
      "of data now available are                                        1  \n",
      "sometimes loosely partly due to                                  0  \n",
      "for large enterprises is determining                             1  \n",
      "many entries (rows) offer greater                                0  \n",
      "...                                                            ...  \n",
      "estimated to reach $215.7 billion                                1  \n",
      "or insightfulness of the data.[5]                                0  \n",
      "data presents challenges in sampling,                            0  \n",
      "devices, aerial (remote sensing) equipment,                      1  \n",
      "analytics methods that extract value                             1  \n",
      "\n",
      "[554 rows x 3 columns]\n",
      "\n",
      "Signature Matrix:\n",
      "                       Hash1  Hash2  Hash3\n",
      "/content/shingle1.txt      0      2      7\n",
      "/content/shingle2.txt      2      4      0\n",
      "/content/shingle3.txt      2      0      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def k_shingles(text, k):\n",
    "    words = text.split()\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = ' '.join(words[i:i + k])\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "# Define the hash functions\n",
    "def hash_function_1(x):\n",
    "    return (3 * x + 5) % 554\n",
    "\n",
    "def hash_function_2(x):\n",
    "    return (7 * x + 4) % 554\n",
    "\n",
    "def hash_function_3(x):\n",
    "    return (3 * x + 1) % 554\n",
    "\n",
    "# File paths\n",
    "file_paths = [\n",
    "    \"/content/shingle1.txt\",\n",
    "    \"/content/shingle2.txt\",\n",
    "    \"/content/shingle3.txt\"\n",
    "]\n",
    "\n",
    "# Token size\n",
    "k = 5\n",
    "\n",
    "# Dictionary to hold shingles for each file\n",
    "shingle_dict = {}\n",
    "\n",
    "# Process each file\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    shingles = k_shingles(content, k)\n",
    "    shingle_dict[file_path] = shingles\n",
    "\n",
    "# Create a set of all unique shingles across all files\n",
    "all_shingles = list(set.union(*[set(shingles) for shingles in shingle_dict.values()]))\n",
    "\n",
    "# Initialize an incidence matrix\n",
    "incidence_matrix = pd.DataFrame(0, index=all_shingles, columns=file_paths)\n",
    "\n",
    "# Populate the incidence matrix\n",
    "for file_path, shingles in shingle_dict.items():\n",
    "    for shingle in shingles:\n",
    "        incidence_matrix.at[shingle, file_path] = 1\n",
    "\n",
    "# Function to compute signatures\n",
    "def compute_signature(incidence_matrix):\n",
    "    signatures = {file_path: [] for file_path in incidence_matrix.columns}\n",
    "\n",
    "    for shingle in incidence_matrix.index:\n",
    "        # Use the index of the shingle in the all_shingles list\n",
    "        index = list(incidence_matrix.index).index(shingle)\n",
    "\n",
    "        for file_path in incidence_matrix.columns:\n",
    "            # Calculate hash values\n",
    "            hash_value = [\n",
    "                hash_function_1(index),\n",
    "                hash_function_2(index),\n",
    "                hash_function_3(index)\n",
    "            ]\n",
    "            # Append hash values if the shingle is present in the file\n",
    "            if incidence_matrix.at[shingle, file_path] == 1:\n",
    "                signatures[file_path].append(hash_value)\n",
    "\n",
    "    # Find the minimum hash value for each hash function per file\n",
    "    final_signatures = {}\n",
    "    for file_path, values in signatures.items():\n",
    "        if values:  # If there are any values\n",
    "            final_signatures[file_path] = [\n",
    "                min(value[0] for value in values),  # Min for Hash1\n",
    "                min(value[1] for value in values),  # Min for Hash2\n",
    "                min(value[2] for value in values)   # Min for Hash3\n",
    "            ]\n",
    "        else:\n",
    "            final_signatures[file_path] = [None, None, None]  # Handle case with no shingles\n",
    "\n",
    "    return pd.DataFrame(final_signatures, index=['Hash1', 'Hash2', 'Hash3']).T\n",
    "\n",
    "# Compute signatures\n",
    "signature_matrix = compute_signature(incidence_matrix)\n",
    "\n",
    "# Display the incidence matrix and signature matrix\n",
    "print(\"Incidence Matrix:\")\n",
    "print(incidence_matrix)\n",
    "\n",
    "print(\"\\nSignature Matrix:\")\n",
    "print(signature_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUzOvNHmDkBc"
   },
   "source": [
    "Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vRSjDILScrN",
    "outputId": "bac284b2-6639-44e0-c99c-9ea564f981cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 7 may be in the set.\n",
      "Element 5 may be in the set.\n"
     ]
    }
   ],
   "source": [
    "class BloomFilter:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.bit_array = [0] * size\n",
    "\n",
    "    def hash1(self, x):\n",
    "        return (x + 1) % self.size\n",
    "\n",
    "    def hash2(self, x):\n",
    "        return (2 * x + 5) % self.size\n",
    "\n",
    "    def add(self, x):\n",
    "        index1 = self.hash1(x)\n",
    "        index2 = self.hash2(x)\n",
    "        self.bit_array[index1] = 1\n",
    "        self.bit_array[index2] = 1\n",
    "\n",
    "    def check(self, x):\n",
    "        index1 = self.hash1(x)\n",
    "        index2 = self.hash2(x)\n",
    "        return self.bit_array[index1] == 1 and self.bit_array[index2] == 1\n",
    "\n",
    "# Initialize Bloom filter\n",
    "bloom_filter = BloomFilter(13)\n",
    "\n",
    "# Add elements 8, 17, 25, 14, 20 to the Bloom filter\n",
    "elements_to_add = [8, 17, 25, 14, 20]\n",
    "for elem in elements_to_add:\n",
    "    bloom_filter.add(elem)\n",
    "\n",
    "# Check for integers 7 and 5\n",
    "check_elements = [7, 5]\n",
    "for elem in check_elements:\n",
    "    if bloom_filter.check(elem):\n",
    "        print(f\"Element {elem} may be in the set.\")\n",
    "    else:\n",
    "        print(f\"Element {elem} is definitely not in the set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeUE2ViYE4iU"
   },
   "source": [
    "AMS(With Given Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a67DpTSONMb_",
    "outputId": "04ac9a60-67d3-4937-fa17-fd059a5a8898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The surprise number (Second Frequency Moment Estimate) is: 162.5\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# AMS Algorithm Implementation\n",
    "def ams_algorithm(stream, x_values):\n",
    "    n = len(stream)\n",
    "\n",
    "    # Initialize the sum of square estimates\n",
    "    sum_squared_estimates = 0\n",
    "\n",
    "    # Perform the AMS estimate for each x_value\n",
    "    for x in x_values:\n",
    "        # Choose a random element r from the stream\n",
    "        r = stream[x - 1]  # x is 1-indexed, so we use x-1 for 0-indexed lists\n",
    "\n",
    "        # Count the number of times r appears in the stream\n",
    "        count_r = stream.count(r)\n",
    "\n",
    "        # Calculate the square of count_r and update the sum of square estimates\n",
    "        sum_squared_estimates += n * (2 * count_r - 1)\n",
    "\n",
    "    # Return the average of the square estimates\n",
    "    return sum_squared_estimates / len(x_values)\n",
    "\n",
    "# Given stream and x values\n",
    "stream = [2, 3, 7, 1, 5, 8, 5, 7, 9, 6, 4, 4, 5, 6, 5, 8, 8, 5,2, 2, 2, 1, 1, 6, 7]\n",
    "x_values = [1, 3, 5, 10]  # Values of x1, x2, x3, and x4 as 1, 3, 5, and 10\n",
    "\n",
    "# Calculate the surprise number using AMS algorithm\n",
    "surprise_number = ams_algorithm(stream, x_values)\n",
    "\n",
    "# Output the result\n",
    "print(\"The surprise number (Second Frequency Moment Estimate) is:\", surprise_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF0S2uHqTIjf"
   },
   "source": [
    "AMS(with Random numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k05t-q70THwZ",
    "outputId": "547ad68d-67cc-4015-cf6c-cf1d4dd97d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random stream is: [7, 10, 6, 8, 8, 6, 7, 6, 2, 3, 7, 9, 4, 8, 1, 7, 10, 8, 1, 9, 6, 5, 7, 9, 5]\n",
      "The surprise number (Second Frequency Moment Estimate) is: 150.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# AMS Algorithm Implementation\n",
    "def ams_algorithm(stream, x_values):\n",
    "    n = len(stream)\n",
    "\n",
    "    # Initialize the sum of square estimates\n",
    "    sum_squared_estimates = 0\n",
    "\n",
    "    # Perform the AMS estimate for each x_value\n",
    "    for x in x_values:\n",
    "        # Choose a random element r from the stream\n",
    "        r = stream[x - 1]  # x is 1-indexed, so we use x-1 for 0-indexed lists\n",
    "\n",
    "        # Count the number of times r appears in the stream\n",
    "        count_r = stream.count(r)\n",
    "\n",
    "        # Calculate the square of count_r and update the sum of square estimates\n",
    "        sum_squared_estimates += n * (2 * count_r - 1)\n",
    "\n",
    "    # Return the average of the square estimates\n",
    "    return sum_squared_estimates / len(x_values)\n",
    "\n",
    "# Generate a random stream of integers\n",
    "random_stream = [random.randint(1, 10) for _ in range(25)]\n",
    "\n",
    "# Given x values\n",
    "x_values = [1, 3, 5, 10]  # Values of x1, x2, x3, and x4 as 1, 3, 5, and 10\n",
    "\n",
    "# Calculate the surprise number using AMS algorithm on the random stream\n",
    "surprise_number = ams_algorithm(random_stream, x_values)\n",
    "\n",
    "# Output the result\n",
    "print(\"The random stream is:\", random_stream)\n",
    "print(\"The surprise number (Second Frequency Moment Estimate) is:\", surprise_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIh3rzDxTYOu"
   },
   "source": [
    "Flajolet Martin algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzLQ2Cw0UJ92",
    "outputId": "9570412b-478c-41d1-d172-ab65144d2a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: 3, Hash: 16, Binary Hash: 10000, Tail Length: 4\n",
      "Element: 1, Hash: 10, Binary Hash: 1010, Tail Length: 1\n",
      "Element: 4, Hash: 19, Binary Hash: 10011, Tail Length: 0\n",
      "Element: 1, Hash: 10, Binary Hash: 1010, Tail Length: 1\n",
      "Element: 5, Hash: 22, Binary Hash: 10110, Tail Length: 1\n",
      "Element: 9, Hash: 2, Binary Hash: 10, Tail Length: 1\n",
      "Element: 2, Hash: 13, Binary Hash: 1101, Tail Length: 0\n",
      "Element: 6, Hash: 25, Binary Hash: 11001, Tail Length: 0\n",
      "Element: 5, Hash: 22, Binary Hash: 10110, Tail Length: 1\n",
      "\n",
      "Estimated number of distinct elements with hash function 3x + 7 mod 32: 16\n",
      "Element: 3, Hash: 7, Binary Hash: 111, Tail Length: 0\n",
      "Element: 1, Hash: 3, Binary Hash: 11, Tail Length: 0\n",
      "Element: 4, Hash: 9, Binary Hash: 1001, Tail Length: 0\n",
      "Element: 1, Hash: 3, Binary Hash: 11, Tail Length: 0\n",
      "Element: 5, Hash: 11, Binary Hash: 1011, Tail Length: 0\n",
      "Element: 9, Hash: 19, Binary Hash: 10011, Tail Length: 0\n",
      "Element: 2, Hash: 5, Binary Hash: 101, Tail Length: 0\n",
      "Element: 6, Hash: 13, Binary Hash: 1101, Tail Length: 0\n",
      "Element: 5, Hash: 11, Binary Hash: 1011, Tail Length: 0\n",
      "\n",
      "Estimated number of distinct elements with hash function 2x + 1 mod 32: 1\n"
     ]
    }
   ],
   "source": [
    "def hash_value(x, a, b, m):\n",
    "    # Hash function: (a*x + b) mod m\n",
    "    return (a * x + b) % m\n",
    "\n",
    "def tail_length(bin_str):\n",
    "    # Find the length of trailing zeros in the binary representation of the hash value\n",
    "    return len(bin_str) - len(bin_str.rstrip('0'))\n",
    "\n",
    "def flajolet_martin(stream, a, b, m):\n",
    "    max_tail_length = 0\n",
    "    tail_lengths = []  # List to store tail lengths for each element\n",
    "\n",
    "    for element in stream:\n",
    "        # Hash the element and convert to binary\n",
    "        hash_val = hash_value(element, a, b, m)\n",
    "        bin_hash = format(hash_val, 'b').zfill(len(bin(hash_val)[2:]))  # Convert to binary and pad with zeros\n",
    "\n",
    "        # Calculate the tail length of the binary hash\n",
    "        t_len = tail_length(bin_hash)\n",
    "        tail_lengths.append((element, hash_val, bin_hash, t_len))  # Store element, hash, and tail length\n",
    "\n",
    "        # Keep track of the maximum tail length\n",
    "        max_tail_length = max(max_tail_length, t_len)\n",
    "\n",
    "    # Print the tail lengths\n",
    "    for elem, h_val, bin_h, t_len in tail_lengths:\n",
    "        print(f\"Element: {elem}, Hash: {h_val}, Binary Hash: {bin_h}, Tail Length: {t_len}\")\n",
    "\n",
    "    # Estimate the number of distinct elements\n",
    "    return 2 ** max_tail_length\n",
    "\n",
    "# Example usage\n",
    "stream = [3, 1, 4, 1, 5, 9, 2, 6, 5]\n",
    "\n",
    "# Use hash function 3x + 7 mod 32\n",
    "a1, b1, m1 = 3, 7, 32\n",
    "estimate1 = flajolet_martin(stream, a1, b1, m1)\n",
    "print(f\"\\nEstimated number of distinct elements with hash function 3x + 7 mod 32: {estimate1}\")\n",
    "\n",
    "# Use hash function 2x + 1 mod 32\n",
    "a2, b2, m2 = 2, 1, 32\n",
    "estimate2 = flajolet_martin(stream, a2, b2, m2)\n",
    "print(f\"\\nEstimated number of distinct elements with hash function 2x + 1 mod 32: {estimate2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhC7YjeRF7bS"
   },
   "source": [
    "Bipartite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW-4NpjnGJNi",
    "outputId": "c823512c-dbff-4ae0-d0f8-30f67b1cd5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Matching: {(4, 'c'), (1, 'a'), (2, 'b')}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Greedy algorithm for bipartite matching\n",
    "class BipartiteMatcher:\n",
    "    def __init__(self, U, V, edges):\n",
    "        # U and V are the sets of vertices in the bipartite graph\n",
    "        # edges is a list of tuples representing edges between U and V\n",
    "        self.U = U\n",
    "        self.V = V\n",
    "        self.edges = edges\n",
    "        self.matching = set()\n",
    "        self.matched_U = set()\n",
    "        self.matched_V = set()\n",
    "\n",
    "    def greedy_match(self):\n",
    "        for u, v in self.edges:\n",
    "            # Check if both u and v are not already matched\n",
    "            if u not in self.matched_U and v not in self.matched_V:\n",
    "                # Add the edge to the matching\n",
    "                self.matching.add((u, v))\n",
    "                # Mark both vertices as matched\n",
    "                self.matched_U.add(u)\n",
    "                self.matched_V.add(v)\n",
    "\n",
    "        return self.matching\n",
    "\n",
    "# Define sets U and V\n",
    "U = {1, 2, 3, 4}  # Set U\n",
    "V = {'a', 'b', 'c', 'd'}  # Set V\n",
    "\n",
    "# List of edges between U and V\n",
    "edges = [(1, 'a'), (2, 'b'), (3, 'a'), (4, 'c'), (2, 'd')]\n",
    "\n",
    "# Create BipartiteMatcher object and find maximal matching\n",
    "matcher = BipartiteMatcher(U, V, edges)\n",
    "matching = matcher.greedy_match()\n",
    "\n",
    "print(\"Maximal Matching:\", matching)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
